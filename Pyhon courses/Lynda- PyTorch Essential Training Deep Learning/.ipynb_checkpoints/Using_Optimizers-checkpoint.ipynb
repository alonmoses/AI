{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HBehvqZ8znsy"
   },
   "source": [
    "# Using optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bCdIqY0tKbvS"
   },
   "outputs": [],
   "source": [
    "# Setting seeds to try and ensure we have the same results - this is not guaranteed across PyTorch releases.\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PCJzXv0OK1Bs"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "mean, std = (0.5,), (0.5,)\n",
    "\n",
    "# Create a transform and normalise data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean, std)\n",
    "                              ])\n",
    "\n",
    "# Download FMNIST training dataset and load training data\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/FMNIST/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download FMNIST test dataset and load test data\n",
    "testset = datasets.FashionMNIST('~/.pytorch/FMNIST/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TZpZ12MrEDZI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rqMqFbIVrbFH"
   },
   "outputs": [],
   "source": [
    "class FMNIST(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.fc1 = nn.Linear(784, 128)\n",
    "    self.fc2 = nn.Linear(128,64)\n",
    "    self.fc3 = nn.Linear(64,10)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    x = x.view(x.shape[0], -1)\n",
    "    \n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = self.fc3(x)\n",
    "    x = F.log_softmax(x, dim=1)\n",
    "    \n",
    "    return x\n",
    "    \n",
    "#model = FMNIST()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m68OeMRdEF0X"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8c0QgxCF3fD-"
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AjBut_7lhAc8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p2ZAGFzFEQA_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-iPQek2nz2yu"
   },
   "outputs": [],
   "source": [
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xMnVwV-CERd_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "roihp-kN0Jw5"
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KvbHIyPSEUPh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gtP3nCEQEUMH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YwcPkxQwEfYX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Nf2WdmP5Gst"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights :  Parameter containing:\n",
      "tensor([[-0.0003,  0.0192, -0.0294,  ...,  0.0219,  0.0037,  0.0021],\n",
      "        [-0.0198, -0.0150, -0.0104,  ..., -0.0203, -0.0060, -0.0299],\n",
      "        [-0.0201,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
      "        ...,\n",
      "        [ 0.0018, -0.0295,  0.0085,  ..., -0.0037,  0.0036,  0.0300],\n",
      "        [-0.0233, -0.0220, -0.0064,  ...,  0.0115, -0.0324, -0.0158],\n",
      "        [ 0.0309,  0.0066,  0.0125,  ...,  0.0286,  0.0350, -0.0105]],\n",
      "       requires_grad=True)\n",
      "Initial weights gradient :  tensor([[-0.0004, -0.0004, -0.0004,  ..., -0.0007, -0.0006, -0.0004],\n",
      "        [ 0.0069,  0.0069,  0.0069,  ...,  0.0072,  0.0070,  0.0069],\n",
      "        [-0.0015, -0.0015, -0.0015,  ..., -0.0016, -0.0015, -0.0015],\n",
      "        ...,\n",
      "        [ 0.0018,  0.0018,  0.0018,  ...,  0.0017,  0.0017,  0.0018],\n",
      "        [ 0.0019,  0.0019,  0.0019,  ...,  0.0019,  0.0019,  0.0019],\n",
      "        [ 0.0017,  0.0017,  0.0017,  ...,  0.0016,  0.0017,  0.0017]])\n"
     ]
    }
   ],
   "source": [
    "output = model(images)\n",
    "loss = criterion(output, labels)\n",
    "loss.backward()\n",
    "print('Initial weights : ',model[0].weight)\n",
    "print('Initial weights gradient : ',model[0].weight.grad)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "arwzAK-1EkEH"
   },
   "outputs": [],
   "source": [
    "optimizer.step() # updating the weights using the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zD-u49yzEj6v"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PuGKi_nq6P0j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights :  Parameter containing:\n",
      "tensor([[-0.0003,  0.0192, -0.0294,  ...,  0.0219,  0.0037,  0.0021],\n",
      "        [-0.0198, -0.0151, -0.0105,  ..., -0.0203, -0.0060, -0.0300],\n",
      "        [-0.0201,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
      "        ...,\n",
      "        [ 0.0018, -0.0296,  0.0085,  ..., -0.0037,  0.0036,  0.0300],\n",
      "        [-0.0233, -0.0221, -0.0064,  ...,  0.0115, -0.0324, -0.0158],\n",
      "        [ 0.0309,  0.0066,  0.0125,  ...,  0.0285,  0.0350, -0.0105]],\n",
      "       requires_grad=True)\n",
      "Initial weights gradient :  tensor([[-0.0004, -0.0004, -0.0004,  ..., -0.0007, -0.0006, -0.0004],\n",
      "        [ 0.0069,  0.0069,  0.0069,  ...,  0.0072,  0.0070,  0.0069],\n",
      "        [-0.0015, -0.0015, -0.0015,  ..., -0.0016, -0.0015, -0.0015],\n",
      "        ...,\n",
      "        [ 0.0018,  0.0018,  0.0018,  ...,  0.0017,  0.0017,  0.0018],\n",
      "        [ 0.0019,  0.0019,  0.0019,  ...,  0.0019,  0.0019,  0.0019],\n",
      "        [ 0.0017,  0.0017,  0.0017,  ...,  0.0016,  0.0017,  0.0017]])\n"
     ]
    }
   ],
   "source": [
    "print('Initial weights : ',model[0].weight)\n",
    "print('Initial weights gradient : ',model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O8oIy5SkEpDn"
   },
   "outputs": [],
   "source": [
    "optimizer.zero_grad() # zeroing the gradient after each calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FnfpzGigEpAr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EniqxHDwDa8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights :  Parameter containing:\n",
      "tensor([[-0.0003,  0.0192, -0.0294,  ...,  0.0219,  0.0037,  0.0021],\n",
      "        [-0.0198, -0.0151, -0.0105,  ..., -0.0203, -0.0060, -0.0300],\n",
      "        [-0.0201,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
      "        ...,\n",
      "        [ 0.0018, -0.0296,  0.0085,  ..., -0.0037,  0.0036,  0.0300],\n",
      "        [-0.0233, -0.0221, -0.0064,  ...,  0.0115, -0.0324, -0.0158],\n",
      "        [ 0.0309,  0.0066,  0.0125,  ...,  0.0285,  0.0350, -0.0105]],\n",
      "       requires_grad=True)\n",
      "Initial weights gradient :  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print('Initial weights : ',model[0].weight)\n",
    "print('Initial weights gradient : ',model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9DViAViGEwyr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mGZhQE3tDcqb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1, Loss : 2.3273115158081055\n",
      "Batch: 2, Loss : 2.293184280395508\n",
      "Batch: 3, Loss : 2.3112661838531494\n",
      "Batch: 4, Loss : 2.3118538856506348\n",
      "Batch: 5, Loss : 2.2956676483154297\n",
      "Batch: 6, Loss : 2.293297529220581\n",
      "Batch: 7, Loss : 2.2968060970306396\n",
      "Batch: 8, Loss : 2.270711660385132\n",
      "Batch: 9, Loss : 2.2949740886688232\n",
      "Batch: 10, Loss : 2.294889450073242\n",
      "Batch: 11, Loss : 2.3100788593292236\n",
      "Batch: 12, Loss : 2.2990283966064453\n",
      "Batch: 13, Loss : 2.2964749336242676\n",
      "Batch: 14, Loss : 2.2953503131866455\n",
      "Batch: 15, Loss : 2.266622543334961\n",
      "Batch: 16, Loss : 2.2846601009368896\n",
      "Batch: 17, Loss : 2.2752439975738525\n",
      "Batch: 18, Loss : 2.261687755584717\n",
      "Batch: 19, Loss : 2.293064832687378\n",
      "Batch: 20, Loss : 2.27963924407959\n",
      "Batch: 21, Loss : 2.255924701690674\n",
      "Batch: 22, Loss : 2.262437343597412\n",
      "Batch: 23, Loss : 2.254652976989746\n",
      "Batch: 24, Loss : 2.2678794860839844\n",
      "Batch: 25, Loss : 2.2677180767059326\n",
      "Batch: 26, Loss : 2.240208387374878\n",
      "Batch: 27, Loss : 2.2844810485839844\n",
      "Batch: 28, Loss : 2.252140522003174\n",
      "Batch: 29, Loss : 2.2464027404785156\n",
      "Batch: 30, Loss : 2.2496814727783203\n",
      "Batch: 31, Loss : 2.255459785461426\n",
      "Batch: 32, Loss : 2.251751184463501\n",
      "Batch: 33, Loss : 2.241237163543701\n",
      "Batch: 34, Loss : 2.24233078956604\n",
      "Batch: 35, Loss : 2.241304874420166\n",
      "Batch: 36, Loss : 2.22330641746521\n",
      "Batch: 37, Loss : 2.223787784576416\n",
      "Batch: 38, Loss : 2.209261655807495\n",
      "Batch: 39, Loss : 2.2302896976470947\n",
      "Batch: 40, Loss : 2.230492115020752\n",
      "Batch: 41, Loss : 2.2180726528167725\n",
      "Batch: 42, Loss : 2.1971888542175293\n",
      "Batch: 43, Loss : 2.2160685062408447\n",
      "Batch: 44, Loss : 2.195664167404175\n",
      "Batch: 45, Loss : 2.1957168579101562\n",
      "Batch: 46, Loss : 2.1685268878936768\n",
      "Batch: 47, Loss : 2.1984055042266846\n",
      "Batch: 48, Loss : 2.1886703968048096\n",
      "Batch: 49, Loss : 2.1618127822875977\n",
      "Batch: 50, Loss : 2.185882329940796\n",
      "Batch: 51, Loss : 2.1491315364837646\n",
      "Batch: 52, Loss : 2.2138187885284424\n",
      "Batch: 53, Loss : 2.1932501792907715\n",
      "Batch: 54, Loss : 2.1595942974090576\n",
      "Batch: 55, Loss : 2.180995464324951\n",
      "Batch: 56, Loss : 2.175490379333496\n",
      "Batch: 57, Loss : 2.1365103721618652\n",
      "Batch: 58, Loss : 2.1840145587921143\n",
      "Batch: 59, Loss : 2.1547746658325195\n",
      "Batch: 60, Loss : 2.152982234954834\n",
      "Batch: 61, Loss : 2.1260910034179688\n",
      "Batch: 62, Loss : 2.1149020195007324\n",
      "Batch: 63, Loss : 2.1577627658843994\n",
      "Batch: 64, Loss : 2.1421492099761963\n",
      "Batch: 65, Loss : 2.1260251998901367\n",
      "Batch: 66, Loss : 2.103114366531372\n",
      "Batch: 67, Loss : 2.1399641036987305\n",
      "Batch: 68, Loss : 2.103525161743164\n",
      "Batch: 69, Loss : 2.141773223876953\n",
      "Batch: 70, Loss : 2.143674850463867\n",
      "Batch: 71, Loss : 2.0862879753112793\n",
      "Batch: 72, Loss : 2.1038362979888916\n",
      "Batch: 73, Loss : 2.1009531021118164\n",
      "Batch: 74, Loss : 2.071605682373047\n",
      "Batch: 75, Loss : 2.0745627880096436\n",
      "Batch: 76, Loss : 2.0910110473632812\n",
      "Batch: 77, Loss : 2.0952603816986084\n",
      "Batch: 78, Loss : 2.1163408756256104\n",
      "Batch: 79, Loss : 2.1010358333587646\n",
      "Batch: 80, Loss : 2.049614906311035\n",
      "Batch: 81, Loss : 2.0651257038116455\n",
      "Batch: 82, Loss : 2.0325682163238525\n",
      "Batch: 83, Loss : 2.066399574279785\n",
      "Batch: 84, Loss : 2.148289918899536\n",
      "Batch: 85, Loss : 2.090207099914551\n",
      "Batch: 86, Loss : 2.0443286895751953\n",
      "Batch: 87, Loss : 2.052696943283081\n",
      "Batch: 88, Loss : 2.026848077774048\n",
      "Batch: 89, Loss : 2.0298914909362793\n",
      "Batch: 90, Loss : 1.9688053131103516\n",
      "Batch: 91, Loss : 2.0118086338043213\n",
      "Batch: 92, Loss : 2.0836970806121826\n",
      "Batch: 93, Loss : 2.010647773742676\n",
      "Batch: 94, Loss : 1.9646961688995361\n",
      "Batch: 95, Loss : 2.0492053031921387\n",
      "Batch: 96, Loss : 1.956960916519165\n",
      "Batch: 97, Loss : 1.970077633857727\n",
      "Batch: 98, Loss : 2.062608480453491\n",
      "Batch: 99, Loss : 2.0435540676116943\n",
      "Batch: 100, Loss : 1.9553219079971313\n",
      "Batch: 101, Loss : 2.0120468139648438\n",
      "Batch: 102, Loss : 2.0084404945373535\n",
      "Batch: 103, Loss : 1.9569793939590454\n",
      "Batch: 104, Loss : 1.95603609085083\n",
      "Batch: 105, Loss : 2.011204719543457\n",
      "Batch: 106, Loss : 1.9950956106185913\n",
      "Batch: 107, Loss : 1.9327149391174316\n",
      "Batch: 108, Loss : 1.9467616081237793\n",
      "Batch: 109, Loss : 1.9839895963668823\n",
      "Batch: 110, Loss : 1.9502699375152588\n",
      "Batch: 111, Loss : 1.927895426750183\n",
      "Batch: 112, Loss : 1.8983615636825562\n",
      "Batch: 113, Loss : 1.9335949420928955\n",
      "Batch: 114, Loss : 1.8573123216629028\n",
      "Batch: 115, Loss : 1.8805265426635742\n",
      "Batch: 116, Loss : 1.9451876878738403\n",
      "Batch: 117, Loss : 1.9638506174087524\n",
      "Batch: 118, Loss : 1.9023414850234985\n",
      "Batch: 119, Loss : 1.9239108562469482\n",
      "Batch: 120, Loss : 1.876333475112915\n",
      "Batch: 121, Loss : 1.923179030418396\n",
      "Batch: 122, Loss : 1.8109216690063477\n",
      "Batch: 123, Loss : 1.842440128326416\n",
      "Batch: 124, Loss : 1.9104013442993164\n",
      "Batch: 125, Loss : 1.8196183443069458\n",
      "Batch: 126, Loss : 1.8708537817001343\n",
      "Batch: 127, Loss : 1.8149341344833374\n",
      "Batch: 128, Loss : 1.8522870540618896\n",
      "Batch: 129, Loss : 1.8546721935272217\n",
      "Batch: 130, Loss : 1.8076210021972656\n",
      "Batch: 131, Loss : 1.9224828481674194\n",
      "Batch: 132, Loss : 1.7730590105056763\n",
      "Batch: 133, Loss : 1.7794853448867798\n",
      "Batch: 134, Loss : 1.8797295093536377\n",
      "Batch: 135, Loss : 1.8297165632247925\n",
      "Batch: 136, Loss : 1.759161114692688\n",
      "Batch: 137, Loss : 1.744276762008667\n",
      "Batch: 138, Loss : 1.8070274591445923\n",
      "Batch: 139, Loss : 1.7021253108978271\n",
      "Batch: 140, Loss : 1.683829426765442\n",
      "Batch: 141, Loss : 1.6387605667114258\n",
      "Batch: 142, Loss : 1.7369858026504517\n",
      "Batch: 143, Loss : 1.8070783615112305\n",
      "Batch: 144, Loss : 1.7636533975601196\n",
      "Batch: 145, Loss : 1.6998896598815918\n",
      "Batch: 146, Loss : 1.7508699893951416\n",
      "Batch: 147, Loss : 1.6857256889343262\n",
      "Batch: 148, Loss : 1.6326251029968262\n",
      "Batch: 149, Loss : 1.7607771158218384\n",
      "Batch: 150, Loss : 1.7454885244369507\n",
      "Batch: 151, Loss : 1.8114913702011108\n",
      "Batch: 152, Loss : 1.8074281215667725\n",
      "Batch: 153, Loss : 1.7459533214569092\n",
      "Batch: 154, Loss : 1.618270754814148\n",
      "Batch: 155, Loss : 1.6749898195266724\n",
      "Batch: 156, Loss : 1.6277682781219482\n",
      "Batch: 157, Loss : 1.5555506944656372\n",
      "Batch: 158, Loss : 1.691530466079712\n",
      "Batch: 159, Loss : 1.7107964754104614\n",
      "Batch: 160, Loss : 1.65519380569458\n",
      "Batch: 161, Loss : 1.596288800239563\n",
      "Batch: 162, Loss : 1.654441475868225\n",
      "Batch: 163, Loss : 1.6711535453796387\n",
      "Batch: 164, Loss : 1.6572893857955933\n",
      "Batch: 165, Loss : 1.6169489622116089\n",
      "Batch: 166, Loss : 1.5714964866638184\n",
      "Batch: 167, Loss : 1.5390191078186035\n",
      "Batch: 168, Loss : 1.6447176933288574\n",
      "Batch: 169, Loss : 1.5337715148925781\n",
      "Batch: 170, Loss : 1.5362979173660278\n",
      "Batch: 171, Loss : 1.5263606309890747\n",
      "Batch: 172, Loss : 1.5534254312515259\n",
      "Batch: 173, Loss : 1.5021554231643677\n",
      "Batch: 174, Loss : 1.4736301898956299\n",
      "Batch: 175, Loss : 1.558469533920288\n",
      "Batch: 176, Loss : 1.4495301246643066\n",
      "Batch: 177, Loss : 1.6550812721252441\n",
      "Batch: 178, Loss : 1.5451717376708984\n",
      "Batch: 179, Loss : 1.5022176504135132\n",
      "Batch: 180, Loss : 1.478644609451294\n",
      "Batch: 181, Loss : 1.4384433031082153\n",
      "Batch: 182, Loss : 1.4458521604537964\n",
      "Batch: 183, Loss : 1.4936569929122925\n",
      "Batch: 184, Loss : 1.6265387535095215\n",
      "Batch: 185, Loss : 1.4937560558319092\n",
      "Batch: 186, Loss : 1.531323790550232\n",
      "Batch: 187, Loss : 1.4495093822479248\n",
      "Batch: 188, Loss : 1.5047180652618408\n",
      "Batch: 189, Loss : 1.5340452194213867\n",
      "Batch: 190, Loss : 1.5063273906707764\n",
      "Batch: 191, Loss : 1.4235426187515259\n",
      "Batch: 192, Loss : 1.4888720512390137\n",
      "Batch: 193, Loss : 1.4857906103134155\n",
      "Batch: 194, Loss : 1.4768178462982178\n",
      "Batch: 195, Loss : 1.3505069017410278\n",
      "Batch: 196, Loss : 1.4549564123153687\n",
      "Batch: 197, Loss : 1.3749921321868896\n",
      "Batch: 198, Loss : 1.3060821294784546\n",
      "Batch: 199, Loss : 1.3821561336517334\n",
      "Batch: 200, Loss : 1.4029353857040405\n",
      "Batch: 201, Loss : 1.422126054763794\n",
      "Batch: 202, Loss : 1.4988561868667603\n",
      "Batch: 203, Loss : 1.4285995960235596\n",
      "Batch: 204, Loss : 1.4050811529159546\n",
      "Batch: 205, Loss : 1.259342908859253\n",
      "Batch: 206, Loss : 1.4864599704742432\n",
      "Batch: 207, Loss : 1.4399893283843994\n",
      "Batch: 208, Loss : 1.3082202672958374\n",
      "Batch: 209, Loss : 1.3609768152236938\n",
      "Batch: 210, Loss : 1.398714303970337\n",
      "Batch: 211, Loss : 1.4033255577087402\n",
      "Batch: 212, Loss : 1.1473619937896729\n",
      "Batch: 213, Loss : 1.4561622142791748\n",
      "Batch: 214, Loss : 1.2958130836486816\n",
      "Batch: 215, Loss : 1.2703216075897217\n",
      "Batch: 216, Loss : 1.3266582489013672\n",
      "Batch: 217, Loss : 1.3298672437667847\n",
      "Batch: 218, Loss : 1.3702744245529175\n",
      "Batch: 219, Loss : 1.3718345165252686\n",
      "Batch: 220, Loss : 1.145382285118103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 221, Loss : 1.3204445838928223\n",
      "Batch: 222, Loss : 1.2740366458892822\n",
      "Batch: 223, Loss : 1.3130650520324707\n",
      "Batch: 224, Loss : 1.2673839330673218\n",
      "Batch: 225, Loss : 1.2436292171478271\n",
      "Batch: 226, Loss : 1.226189374923706\n",
      "Batch: 227, Loss : 1.2443219423294067\n",
      "Batch: 228, Loss : 1.3343570232391357\n",
      "Batch: 229, Loss : 1.1850758790969849\n",
      "Batch: 230, Loss : 1.3133430480957031\n",
      "Batch: 231, Loss : 1.318487524986267\n",
      "Batch: 232, Loss : 1.4127391576766968\n",
      "Batch: 233, Loss : 1.3631834983825684\n",
      "Batch: 234, Loss : 1.316428780555725\n",
      "Batch: 235, Loss : 1.2542226314544678\n",
      "Batch: 236, Loss : 1.312017560005188\n",
      "Batch: 237, Loss : 1.2638838291168213\n",
      "Batch: 238, Loss : 1.2465494871139526\n",
      "Batch: 239, Loss : 1.2897800207138062\n",
      "Batch: 240, Loss : 1.2794121503829956\n",
      "Batch: 241, Loss : 1.194608211517334\n",
      "Batch: 242, Loss : 1.2498332262039185\n",
      "Batch: 243, Loss : 1.1584415435791016\n",
      "Batch: 244, Loss : 1.1943132877349854\n",
      "Batch: 245, Loss : 1.2157005071640015\n",
      "Batch: 246, Loss : 1.2400010824203491\n",
      "Batch: 247, Loss : 1.1818710565567017\n",
      "Batch: 248, Loss : 1.090733528137207\n",
      "Batch: 249, Loss : 1.1674489974975586\n",
      "Batch: 250, Loss : 1.1251929998397827\n",
      "Batch: 251, Loss : 1.2300260066986084\n",
      "Batch: 252, Loss : 1.0250967741012573\n",
      "Batch: 253, Loss : 1.1399164199829102\n",
      "Batch: 254, Loss : 1.1958332061767578\n",
      "Batch: 255, Loss : 1.0432047843933105\n",
      "Batch: 256, Loss : 1.0621286630630493\n",
      "Batch: 257, Loss : 1.199872374534607\n",
      "Batch: 258, Loss : 1.0784401893615723\n",
      "Batch: 259, Loss : 1.065917730331421\n",
      "Batch: 260, Loss : 1.137269377708435\n",
      "Batch: 261, Loss : 1.1109539270401\n",
      "Batch: 262, Loss : 1.1976873874664307\n",
      "Batch: 263, Loss : 1.1297409534454346\n",
      "Batch: 264, Loss : 1.1072369813919067\n",
      "Batch: 265, Loss : 1.108964443206787\n",
      "Batch: 266, Loss : 1.1546357870101929\n",
      "Batch: 267, Loss : 1.186678409576416\n",
      "Batch: 268, Loss : 1.1612697839736938\n",
      "Batch: 269, Loss : 1.1416951417922974\n",
      "Batch: 270, Loss : 1.1570370197296143\n",
      "Batch: 271, Loss : 1.0596108436584473\n",
      "Batch: 272, Loss : 1.2348862886428833\n",
      "Batch: 273, Loss : 0.969756543636322\n",
      "Batch: 274, Loss : 1.0958589315414429\n",
      "Batch: 275, Loss : 1.123828411102295\n",
      "Batch: 276, Loss : 1.1918036937713623\n",
      "Batch: 277, Loss : 1.069437026977539\n",
      "Batch: 278, Loss : 1.100964903831482\n",
      "Batch: 279, Loss : 1.1134462356567383\n",
      "Batch: 280, Loss : 1.1238844394683838\n",
      "Batch: 281, Loss : 1.0628960132598877\n",
      "Batch: 282, Loss : 1.0466152429580688\n",
      "Batch: 283, Loss : 1.1142370700836182\n",
      "Batch: 284, Loss : 1.0152366161346436\n",
      "Batch: 285, Loss : 1.0667567253112793\n",
      "Batch: 286, Loss : 0.9787743091583252\n",
      "Batch: 287, Loss : 0.9881272315979004\n",
      "Batch: 288, Loss : 1.1986291408538818\n",
      "Batch: 289, Loss : 0.9701381325721741\n",
      "Batch: 290, Loss : 1.0911551713943481\n",
      "Batch: 291, Loss : 1.1162701845169067\n",
      "Batch: 292, Loss : 1.1162831783294678\n",
      "Batch: 293, Loss : 1.0415209531784058\n",
      "Batch: 294, Loss : 0.9752520322799683\n",
      "Batch: 295, Loss : 0.9725850224494934\n",
      "Batch: 296, Loss : 1.0996334552764893\n",
      "Batch: 297, Loss : 0.9933522343635559\n",
      "Batch: 298, Loss : 1.0123423337936401\n",
      "Batch: 299, Loss : 1.1646250486373901\n",
      "Batch: 300, Loss : 0.9986432790756226\n",
      "Batch: 301, Loss : 1.1009727716445923\n",
      "Batch: 302, Loss : 1.0020027160644531\n",
      "Batch: 303, Loss : 1.0542353391647339\n",
      "Batch: 304, Loss : 1.0283277034759521\n",
      "Batch: 305, Loss : 0.9996098279953003\n",
      "Batch: 306, Loss : 1.016979694366455\n",
      "Batch: 307, Loss : 1.1654508113861084\n",
      "Batch: 308, Loss : 0.9684611558914185\n",
      "Batch: 309, Loss : 1.0222221612930298\n",
      "Batch: 310, Loss : 1.053237795829773\n",
      "Batch: 311, Loss : 1.2060441970825195\n",
      "Batch: 312, Loss : 1.118337631225586\n",
      "Batch: 313, Loss : 1.0477070808410645\n",
      "Batch: 314, Loss : 0.9664521813392639\n",
      "Batch: 315, Loss : 0.8346862196922302\n",
      "Batch: 316, Loss : 1.0226577520370483\n",
      "Batch: 317, Loss : 0.9046198725700378\n",
      "Batch: 318, Loss : 1.000677466392517\n",
      "Batch: 319, Loss : 0.9734113216400146\n",
      "Batch: 320, Loss : 1.0436793565750122\n",
      "Batch: 321, Loss : 0.9620984196662903\n",
      "Batch: 322, Loss : 0.8919892907142639\n",
      "Batch: 323, Loss : 0.9384574294090271\n",
      "Batch: 324, Loss : 1.1045819520950317\n",
      "Batch: 325, Loss : 0.9861093759536743\n",
      "Batch: 326, Loss : 0.9690812826156616\n",
      "Batch: 327, Loss : 1.0247929096221924\n",
      "Batch: 328, Loss : 0.9651250839233398\n",
      "Batch: 329, Loss : 0.9823943376541138\n",
      "Batch: 330, Loss : 0.9871727824211121\n",
      "Batch: 331, Loss : 1.0445858240127563\n",
      "Batch: 332, Loss : 0.8640491366386414\n",
      "Batch: 333, Loss : 0.9529538154602051\n",
      "Batch: 334, Loss : 0.9570234417915344\n",
      "Batch: 335, Loss : 1.0511647462844849\n",
      "Batch: 336, Loss : 0.9836487770080566\n",
      "Batch: 337, Loss : 0.9255005717277527\n",
      "Batch: 338, Loss : 1.0289306640625\n",
      "Batch: 339, Loss : 1.071473479270935\n",
      "Batch: 340, Loss : 0.9118156433105469\n",
      "Batch: 341, Loss : 0.9953745603561401\n",
      "Batch: 342, Loss : 0.9982455968856812\n",
      "Batch: 343, Loss : 0.9288923144340515\n",
      "Batch: 344, Loss : 1.1023085117340088\n",
      "Batch: 345, Loss : 0.9437663555145264\n",
      "Batch: 346, Loss : 1.0315576791763306\n",
      "Batch: 347, Loss : 0.9392939209938049\n",
      "Batch: 348, Loss : 1.0071433782577515\n",
      "Batch: 349, Loss : 1.0192205905914307\n",
      "Batch: 350, Loss : 1.0599912405014038\n",
      "Batch: 351, Loss : 1.0648407936096191\n",
      "Batch: 352, Loss : 0.8403050303459167\n",
      "Batch: 353, Loss : 0.9934825301170349\n",
      "Batch: 354, Loss : 0.8439375162124634\n",
      "Batch: 355, Loss : 0.9413086175918579\n",
      "Batch: 356, Loss : 0.8351964354515076\n",
      "Batch: 357, Loss : 0.9771080017089844\n",
      "Batch: 358, Loss : 0.9214141368865967\n",
      "Batch: 359, Loss : 1.0070475339889526\n",
      "Batch: 360, Loss : 0.7367008328437805\n",
      "Batch: 361, Loss : 0.8449763655662537\n",
      "Batch: 362, Loss : 0.982343316078186\n",
      "Batch: 363, Loss : 0.9421733021736145\n",
      "Batch: 364, Loss : 0.9092972874641418\n",
      "Batch: 365, Loss : 1.0718677043914795\n",
      "Batch: 366, Loss : 0.9499006271362305\n",
      "Batch: 367, Loss : 0.9802025556564331\n",
      "Batch: 368, Loss : 0.9553020596504211\n",
      "Batch: 369, Loss : 0.9584803581237793\n",
      "Batch: 370, Loss : 0.9101927876472473\n",
      "Batch: 371, Loss : 0.939177393913269\n",
      "Batch: 372, Loss : 0.922457754611969\n",
      "Batch: 373, Loss : 1.1122301816940308\n",
      "Batch: 374, Loss : 0.9095723628997803\n",
      "Batch: 375, Loss : 0.8718498945236206\n",
      "Batch: 376, Loss : 0.8855853080749512\n",
      "Batch: 377, Loss : 0.9148469567298889\n",
      "Batch: 378, Loss : 0.9749128818511963\n",
      "Batch: 379, Loss : 0.981246829032898\n",
      "Batch: 380, Loss : 0.9354064464569092\n",
      "Batch: 381, Loss : 0.9981544613838196\n",
      "Batch: 382, Loss : 0.8749374151229858\n",
      "Batch: 383, Loss : 1.0383564233779907\n",
      "Batch: 384, Loss : 0.8711704015731812\n",
      "Batch: 385, Loss : 0.8778778314590454\n",
      "Batch: 386, Loss : 0.8898894190788269\n",
      "Batch: 387, Loss : 0.9857416152954102\n",
      "Batch: 388, Loss : 0.9601754546165466\n",
      "Batch: 389, Loss : 0.9529748558998108\n",
      "Batch: 390, Loss : 0.8925163745880127\n",
      "Batch: 391, Loss : 0.9820724129676819\n",
      "Batch: 392, Loss : 0.8238970637321472\n",
      "Batch: 393, Loss : 0.9577304124832153\n",
      "Batch: 394, Loss : 0.9329569935798645\n",
      "Batch: 395, Loss : 0.8806898593902588\n",
      "Batch: 396, Loss : 0.9192200899124146\n",
      "Batch: 397, Loss : 0.8574773073196411\n",
      "Batch: 398, Loss : 0.9403814673423767\n",
      "Batch: 399, Loss : 0.8666718602180481\n",
      "Batch: 400, Loss : 0.8554797172546387\n",
      "Batch: 401, Loss : 0.8636573553085327\n",
      "Batch: 402, Loss : 1.0034083127975464\n",
      "Batch: 403, Loss : 0.7862417697906494\n",
      "Batch: 404, Loss : 0.921034038066864\n",
      "Batch: 405, Loss : 0.9427769184112549\n",
      "Batch: 406, Loss : 0.8704990148544312\n",
      "Batch: 407, Loss : 0.9049021601676941\n",
      "Batch: 408, Loss : 0.8576884269714355\n",
      "Batch: 409, Loss : 0.7972546219825745\n",
      "Batch: 410, Loss : 0.8513543009757996\n",
      "Batch: 411, Loss : 0.7885668873786926\n",
      "Batch: 412, Loss : 0.7859570980072021\n",
      "Batch: 413, Loss : 0.820963442325592\n",
      "Batch: 414, Loss : 0.9094308018684387\n",
      "Batch: 415, Loss : 0.802445113658905\n",
      "Batch: 416, Loss : 0.8691621422767639\n",
      "Batch: 417, Loss : 0.795307993888855\n",
      "Batch: 418, Loss : 0.7965037226676941\n",
      "Batch: 419, Loss : 0.8437089920043945\n",
      "Batch: 420, Loss : 0.7596662044525146\n",
      "Batch: 421, Loss : 0.9215367436408997\n",
      "Batch: 422, Loss : 0.8011173605918884\n",
      "Batch: 423, Loss : 0.7975947260856628\n",
      "Batch: 424, Loss : 1.011278748512268\n",
      "Batch: 425, Loss : 0.797141432762146\n",
      "Batch: 426, Loss : 0.994014322757721\n",
      "Batch: 427, Loss : 0.8279837369918823\n",
      "Batch: 428, Loss : 0.7668923735618591\n",
      "Batch: 429, Loss : 0.8767348527908325\n",
      "Batch: 430, Loss : 0.8824460506439209\n",
      "Batch: 431, Loss : 0.7527403235435486\n",
      "Batch: 432, Loss : 0.8126322627067566\n",
      "Batch: 433, Loss : 0.8151460289955139\n",
      "Batch: 434, Loss : 0.607306957244873\n",
      "Batch: 435, Loss : 0.8183320164680481\n",
      "Batch: 436, Loss : 0.800838828086853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 437, Loss : 0.7536025047302246\n",
      "Batch: 438, Loss : 0.7344563007354736\n",
      "Batch: 439, Loss : 0.7935022115707397\n",
      "Batch: 440, Loss : 0.8040387034416199\n",
      "Batch: 441, Loss : 0.8050840497016907\n",
      "Batch: 442, Loss : 0.8211210370063782\n",
      "Batch: 443, Loss : 0.8516119122505188\n",
      "Batch: 444, Loss : 0.7371495366096497\n",
      "Batch: 445, Loss : 0.6923138499259949\n",
      "Batch: 446, Loss : 0.8441256880760193\n",
      "Batch: 447, Loss : 0.7863665223121643\n",
      "Batch: 448, Loss : 0.7977224588394165\n",
      "Batch: 449, Loss : 0.8043186068534851\n",
      "Batch: 450, Loss : 0.6792334318161011\n",
      "Batch: 451, Loss : 0.8397294282913208\n",
      "Batch: 452, Loss : 0.8697363138198853\n",
      "Batch: 453, Loss : 0.771031379699707\n",
      "Batch: 454, Loss : 0.8126727342605591\n",
      "Batch: 455, Loss : 0.6270816326141357\n",
      "Batch: 456, Loss : 0.8570786118507385\n",
      "Batch: 457, Loss : 0.842613697052002\n",
      "Batch: 458, Loss : 0.8717027306556702\n",
      "Batch: 459, Loss : 0.7391253709793091\n",
      "Batch: 460, Loss : 0.7095186710357666\n",
      "Batch: 461, Loss : 0.842334508895874\n",
      "Batch: 462, Loss : 0.647293210029602\n",
      "Batch: 463, Loss : 0.8582839369773865\n",
      "Batch: 464, Loss : 0.7238839864730835\n",
      "Batch: 465, Loss : 0.7631592154502869\n",
      "Batch: 466, Loss : 0.8765307664871216\n",
      "Batch: 467, Loss : 0.841210663318634\n",
      "Batch: 468, Loss : 0.7760308384895325\n",
      "Batch: 469, Loss : 0.8015806078910828\n",
      "Batch: 470, Loss : 0.6863647103309631\n",
      "Batch: 471, Loss : 0.8473804593086243\n",
      "Batch: 472, Loss : 0.9001515507698059\n",
      "Batch: 473, Loss : 0.7945055961608887\n",
      "Batch: 474, Loss : 0.9148767590522766\n",
      "Batch: 475, Loss : 0.8279215693473816\n",
      "Batch: 476, Loss : 0.7155454158782959\n",
      "Batch: 477, Loss : 0.7941468358039856\n",
      "Batch: 478, Loss : 0.7993265986442566\n",
      "Batch: 479, Loss : 0.8518549203872681\n",
      "Batch: 480, Loss : 0.9756640195846558\n",
      "Batch: 481, Loss : 0.7060246467590332\n",
      "Batch: 482, Loss : 0.8207886219024658\n",
      "Batch: 483, Loss : 0.8095158338546753\n",
      "Batch: 484, Loss : 0.7697265148162842\n",
      "Batch: 485, Loss : 0.7551531195640564\n",
      "Batch: 486, Loss : 0.9006812572479248\n",
      "Batch: 487, Loss : 0.7239061594009399\n",
      "Batch: 488, Loss : 0.6565117239952087\n",
      "Batch: 489, Loss : 0.8251330852508545\n",
      "Batch: 490, Loss : 0.7854807376861572\n",
      "Batch: 491, Loss : 0.7853288054466248\n",
      "Batch: 492, Loss : 0.8286432027816772\n",
      "Batch: 493, Loss : 0.6997424364089966\n",
      "Batch: 494, Loss : 0.8673913478851318\n",
      "Batch: 495, Loss : 0.7331609725952148\n",
      "Batch: 496, Loss : 0.7486644387245178\n",
      "Batch: 497, Loss : 0.9435230493545532\n",
      "Batch: 498, Loss : 0.876980185508728\n",
      "Batch: 499, Loss : 0.7139675617218018\n",
      "Batch: 500, Loss : 0.7867436408996582\n",
      "Batch: 501, Loss : 0.8134593367576599\n",
      "Batch: 502, Loss : 0.7951771020889282\n",
      "Batch: 503, Loss : 0.8110448122024536\n",
      "Batch: 504, Loss : 0.7855314612388611\n",
      "Batch: 505, Loss : 0.5682210922241211\n",
      "Batch: 506, Loss : 0.6269574165344238\n",
      "Batch: 507, Loss : 0.8924821615219116\n",
      "Batch: 508, Loss : 0.8022152185440063\n",
      "Batch: 509, Loss : 0.8146771192550659\n",
      "Batch: 510, Loss : 0.9540273547172546\n",
      "Batch: 511, Loss : 1.002563238143921\n",
      "Batch: 512, Loss : 0.7074458003044128\n",
      "Batch: 513, Loss : 0.9084703326225281\n",
      "Batch: 514, Loss : 0.6224662661552429\n",
      "Batch: 515, Loss : 0.8382639288902283\n",
      "Batch: 516, Loss : 0.8228788375854492\n",
      "Batch: 517, Loss : 0.7183207273483276\n",
      "Batch: 518, Loss : 0.8823406100273132\n",
      "Batch: 519, Loss : 0.8171542882919312\n",
      "Batch: 520, Loss : 0.6724094748497009\n",
      "Batch: 521, Loss : 0.8087440133094788\n",
      "Batch: 522, Loss : 0.7228416204452515\n",
      "Batch: 523, Loss : 0.9134393930435181\n",
      "Batch: 524, Loss : 0.7112213373184204\n",
      "Batch: 525, Loss : 0.8290936946868896\n",
      "Batch: 526, Loss : 0.8415986895561218\n",
      "Batch: 527, Loss : 0.7613736391067505\n",
      "Batch: 528, Loss : 0.6551432013511658\n",
      "Batch: 529, Loss : 0.8264554738998413\n",
      "Batch: 530, Loss : 0.6280972957611084\n",
      "Batch: 531, Loss : 0.8430714011192322\n",
      "Batch: 532, Loss : 0.6474553942680359\n",
      "Batch: 533, Loss : 0.6893207430839539\n",
      "Batch: 534, Loss : 0.821264386177063\n",
      "Batch: 535, Loss : 0.9134467244148254\n",
      "Batch: 536, Loss : 1.0133849382400513\n",
      "Batch: 537, Loss : 0.7656716108322144\n",
      "Batch: 538, Loss : 0.7302987575531006\n",
      "Batch: 539, Loss : 0.8212284445762634\n",
      "Batch: 540, Loss : 0.8651366829872131\n",
      "Batch: 541, Loss : 0.7862277030944824\n",
      "Batch: 542, Loss : 0.7855923175811768\n",
      "Batch: 543, Loss : 0.7764663696289062\n",
      "Batch: 544, Loss : 0.7485922574996948\n",
      "Batch: 545, Loss : 0.8592132329940796\n",
      "Batch: 546, Loss : 0.7926104664802551\n",
      "Batch: 547, Loss : 0.8510979413986206\n",
      "Batch: 548, Loss : 0.7837697863578796\n",
      "Batch: 549, Loss : 0.6937589049339294\n",
      "Batch: 550, Loss : 0.6880781054496765\n",
      "Batch: 551, Loss : 0.8868483901023865\n",
      "Batch: 552, Loss : 0.6922814249992371\n",
      "Batch: 553, Loss : 0.602690577507019\n",
      "Batch: 554, Loss : 0.7744850516319275\n",
      "Batch: 555, Loss : 0.6752322316169739\n",
      "Batch: 556, Loss : 0.7375317811965942\n",
      "Batch: 557, Loss : 0.6026614308357239\n",
      "Batch: 558, Loss : 0.7372385263442993\n",
      "Batch: 559, Loss : 0.6876018643379211\n",
      "Batch: 560, Loss : 0.7780503034591675\n",
      "Batch: 561, Loss : 0.8482325673103333\n",
      "Batch: 562, Loss : 0.7800635695457458\n",
      "Batch: 563, Loss : 0.8750579357147217\n",
      "Batch: 564, Loss : 0.7080211639404297\n",
      "Batch: 565, Loss : 0.9004775285720825\n",
      "Batch: 566, Loss : 0.6859767436981201\n",
      "Batch: 567, Loss : 0.6804129481315613\n",
      "Batch: 568, Loss : 0.7694492936134338\n",
      "Batch: 569, Loss : 0.8353064656257629\n",
      "Batch: 570, Loss : 0.6751993894577026\n",
      "Batch: 571, Loss : 0.7639975547790527\n",
      "Batch: 572, Loss : 0.7527135014533997\n",
      "Batch: 573, Loss : 0.7043353319168091\n",
      "Batch: 574, Loss : 0.7168086171150208\n",
      "Batch: 575, Loss : 0.796330988407135\n",
      "Batch: 576, Loss : 0.6002797484397888\n",
      "Batch: 577, Loss : 0.8390539884567261\n",
      "Batch: 578, Loss : 0.6267789006233215\n",
      "Batch: 579, Loss : 0.6468489766120911\n",
      "Batch: 580, Loss : 0.750808835029602\n",
      "Batch: 581, Loss : 0.592311441898346\n",
      "Batch: 582, Loss : 0.8506571650505066\n",
      "Batch: 583, Loss : 0.6922179460525513\n",
      "Batch: 584, Loss : 0.9217705130577087\n",
      "Batch: 585, Loss : 0.6976444721221924\n",
      "Batch: 586, Loss : 0.7630594372749329\n",
      "Batch: 587, Loss : 0.6331157684326172\n",
      "Batch: 588, Loss : 0.6442298293113708\n",
      "Batch: 589, Loss : 0.8779240846633911\n",
      "Batch: 590, Loss : 0.7617102265357971\n",
      "Batch: 591, Loss : 0.7693935036659241\n",
      "Batch: 592, Loss : 0.6655199527740479\n",
      "Batch: 593, Loss : 0.743015706539154\n",
      "Batch: 594, Loss : 0.8678366541862488\n",
      "Batch: 595, Loss : 0.6739274263381958\n",
      "Batch: 596, Loss : 0.662092924118042\n",
      "Batch: 597, Loss : 0.6911786794662476\n",
      "Batch: 598, Loss : 0.6427634358406067\n",
      "Batch: 599, Loss : 0.8190838098526001\n",
      "Batch: 600, Loss : 0.6754717826843262\n",
      "Batch: 601, Loss : 0.7521235942840576\n",
      "Batch: 602, Loss : 0.909469485282898\n",
      "Batch: 603, Loss : 0.763068675994873\n",
      "Batch: 604, Loss : 0.6241615414619446\n",
      "Batch: 605, Loss : 0.6525794267654419\n",
      "Batch: 606, Loss : 0.6715345978736877\n",
      "Batch: 607, Loss : 0.7381927967071533\n",
      "Batch: 608, Loss : 0.6565983891487122\n",
      "Batch: 609, Loss : 0.8577746748924255\n",
      "Batch: 610, Loss : 0.8465964198112488\n",
      "Batch: 611, Loss : 0.8585231900215149\n",
      "Batch: 612, Loss : 0.5577929615974426\n",
      "Batch: 613, Loss : 0.7139306664466858\n",
      "Batch: 614, Loss : 0.5755378603935242\n",
      "Batch: 615, Loss : 0.731644868850708\n",
      "Batch: 616, Loss : 0.6866051554679871\n",
      "Batch: 617, Loss : 0.9120909571647644\n",
      "Batch: 618, Loss : 0.5945602059364319\n",
      "Batch: 619, Loss : 0.7434573173522949\n",
      "Batch: 620, Loss : 0.6608197093009949\n",
      "Batch: 621, Loss : 0.8504916429519653\n",
      "Batch: 622, Loss : 0.7011074423789978\n",
      "Batch: 623, Loss : 0.6219558119773865\n",
      "Batch: 624, Loss : 0.5793281197547913\n",
      "Batch: 625, Loss : 0.6784803867340088\n",
      "Batch: 626, Loss : 0.6754632592201233\n",
      "Batch: 627, Loss : 0.6767398118972778\n",
      "Batch: 628, Loss : 0.6994289755821228\n",
      "Batch: 629, Loss : 0.7688103318214417\n",
      "Batch: 630, Loss : 0.6751328706741333\n",
      "Batch: 631, Loss : 0.5783237814903259\n",
      "Batch: 632, Loss : 0.6229199171066284\n",
      "Batch: 633, Loss : 0.5994378328323364\n",
      "Batch: 634, Loss : 0.8890005350112915\n",
      "Batch: 635, Loss : 0.7378740906715393\n",
      "Batch: 636, Loss : 0.9458001852035522\n",
      "Batch: 637, Loss : 0.7082721590995789\n",
      "Batch: 638, Loss : 0.6691229343414307\n",
      "Batch: 639, Loss : 0.7861568927764893\n",
      "Batch: 640, Loss : 0.8284801244735718\n",
      "Batch: 641, Loss : 0.6616047620773315\n",
      "Batch: 642, Loss : 0.796766996383667\n",
      "Batch: 643, Loss : 0.5121679902076721\n",
      "Batch: 644, Loss : 0.7617524266242981\n",
      "Batch: 645, Loss : 0.5821793079376221\n",
      "Batch: 646, Loss : 0.7170842289924622\n",
      "Batch: 647, Loss : 0.8153561949729919\n",
      "Batch: 648, Loss : 0.6911876797676086\n",
      "Batch: 649, Loss : 0.9210999011993408\n",
      "Batch: 650, Loss : 0.6813797354698181\n",
      "Batch: 651, Loss : 0.7433317303657532\n",
      "Batch: 652, Loss : 0.5179157257080078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 653, Loss : 0.7317698001861572\n",
      "Batch: 654, Loss : 0.7479687929153442\n",
      "Batch: 655, Loss : 0.8091865181922913\n",
      "Batch: 656, Loss : 0.7208266854286194\n",
      "Batch: 657, Loss : 0.6284978985786438\n",
      "Batch: 658, Loss : 0.6035612225532532\n",
      "Batch: 659, Loss : 0.6831122040748596\n",
      "Batch: 660, Loss : 0.6508500576019287\n",
      "Batch: 661, Loss : 0.8517587184906006\n",
      "Batch: 662, Loss : 0.708644449710846\n",
      "Batch: 663, Loss : 0.7897759675979614\n",
      "Batch: 664, Loss : 0.7139507532119751\n",
      "Batch: 665, Loss : 0.6474398970603943\n",
      "Batch: 666, Loss : 0.6890150904655457\n",
      "Batch: 667, Loss : 0.7021757960319519\n",
      "Batch: 668, Loss : 0.7152019143104553\n",
      "Batch: 669, Loss : 0.6328343749046326\n",
      "Batch: 670, Loss : 0.788378894329071\n",
      "Batch: 671, Loss : 0.6253613829612732\n",
      "Batch: 672, Loss : 0.6547462940216064\n",
      "Batch: 673, Loss : 0.6393874287605286\n",
      "Batch: 674, Loss : 0.8240779042243958\n",
      "Batch: 675, Loss : 0.7207798361778259\n",
      "Batch: 676, Loss : 0.7777431011199951\n",
      "Batch: 677, Loss : 0.7428613305091858\n",
      "Batch: 678, Loss : 0.7353305816650391\n",
      "Batch: 679, Loss : 0.6703954935073853\n",
      "Batch: 680, Loss : 0.7006705403327942\n",
      "Batch: 681, Loss : 0.6558612585067749\n",
      "Batch: 682, Loss : 0.7063937783241272\n",
      "Batch: 683, Loss : 0.8117830157279968\n",
      "Batch: 684, Loss : 0.6920735239982605\n",
      "Batch: 685, Loss : 0.7411488890647888\n",
      "Batch: 686, Loss : 0.7310933470726013\n",
      "Batch: 687, Loss : 0.8177555203437805\n",
      "Batch: 688, Loss : 0.908250093460083\n",
      "Batch: 689, Loss : 0.9411774277687073\n",
      "Batch: 690, Loss : 0.7364851236343384\n",
      "Batch: 691, Loss : 0.8057397603988647\n",
      "Batch: 692, Loss : 0.7336402535438538\n",
      "Batch: 693, Loss : 0.6875914931297302\n",
      "Batch: 694, Loss : 0.7330647110939026\n",
      "Batch: 695, Loss : 0.808738648891449\n",
      "Batch: 696, Loss : 0.7333459258079529\n",
      "Batch: 697, Loss : 0.8746303915977478\n",
      "Batch: 698, Loss : 0.6052054166793823\n",
      "Batch: 699, Loss : 0.8519527912139893\n",
      "Batch: 700, Loss : 0.6936705708503723\n",
      "Batch: 701, Loss : 0.7707494497299194\n",
      "Batch: 702, Loss : 0.6335062980651855\n",
      "Batch: 703, Loss : 0.6772376298904419\n",
      "Batch: 704, Loss : 0.7080058455467224\n",
      "Batch: 705, Loss : 0.7527475357055664\n",
      "Batch: 706, Loss : 0.6065925359725952\n",
      "Batch: 707, Loss : 0.6377304792404175\n",
      "Batch: 708, Loss : 0.9024211168289185\n",
      "Batch: 709, Loss : 0.8189219832420349\n",
      "Batch: 710, Loss : 0.7374135255813599\n",
      "Batch: 711, Loss : 0.6143262386322021\n",
      "Batch: 712, Loss : 0.5497864484786987\n",
      "Batch: 713, Loss : 0.8488949537277222\n",
      "Batch: 714, Loss : 0.8693807721138\n",
      "Batch: 715, Loss : 0.7122184038162231\n",
      "Batch: 716, Loss : 0.7681177258491516\n",
      "Batch: 717, Loss : 0.6535243391990662\n",
      "Batch: 718, Loss : 0.6403048634529114\n",
      "Batch: 719, Loss : 0.7696778774261475\n",
      "Batch: 720, Loss : 0.6678258776664734\n",
      "Batch: 721, Loss : 0.5137903094291687\n",
      "Batch: 722, Loss : 0.8081006407737732\n",
      "Batch: 723, Loss : 0.8084613680839539\n",
      "Batch: 724, Loss : 0.7414115071296692\n",
      "Batch: 725, Loss : 0.7732499837875366\n",
      "Batch: 726, Loss : 0.6630882024765015\n",
      "Batch: 727, Loss : 0.7614527940750122\n",
      "Batch: 728, Loss : 0.5605586767196655\n",
      "Batch: 729, Loss : 0.6925650835037231\n",
      "Batch: 730, Loss : 0.7569970488548279\n",
      "Batch: 731, Loss : 0.6323879957199097\n",
      "Batch: 732, Loss : 0.6035411953926086\n",
      "Batch: 733, Loss : 0.5867924690246582\n",
      "Batch: 734, Loss : 0.5775707364082336\n",
      "Batch: 735, Loss : 0.7435669302940369\n",
      "Batch: 736, Loss : 0.8573669195175171\n",
      "Batch: 737, Loss : 0.8533198833465576\n",
      "Batch: 738, Loss : 0.7395967841148376\n",
      "Batch: 739, Loss : 0.5989599823951721\n",
      "Batch: 740, Loss : 0.5641716718673706\n",
      "Batch: 741, Loss : 0.7083389759063721\n",
      "Batch: 742, Loss : 0.6836386322975159\n",
      "Batch: 743, Loss : 0.6073178052902222\n",
      "Batch: 744, Loss : 0.7224851846694946\n",
      "Batch: 745, Loss : 0.5693383812904358\n",
      "Batch: 746, Loss : 0.7741822004318237\n",
      "Batch: 747, Loss : 0.6510053277015686\n",
      "Batch: 748, Loss : 0.9085187911987305\n",
      "Batch: 749, Loss : 0.5224825143814087\n",
      "Batch: 750, Loss : 0.5554496049880981\n",
      "Batch: 751, Loss : 0.7256032824516296\n",
      "Batch: 752, Loss : 0.5776955485343933\n",
      "Batch: 753, Loss : 0.6473879218101501\n",
      "Batch: 754, Loss : 0.5489739179611206\n",
      "Batch: 755, Loss : 0.7437781691551208\n",
      "Batch: 756, Loss : 0.6338275671005249\n",
      "Batch: 757, Loss : 0.6045593023300171\n",
      "Batch: 758, Loss : 0.6054053902626038\n",
      "Batch: 759, Loss : 0.6632404327392578\n",
      "Batch: 760, Loss : 0.532269299030304\n",
      "Batch: 761, Loss : 0.6758880019187927\n",
      "Batch: 762, Loss : 0.8258281946182251\n",
      "Batch: 763, Loss : 0.5173543691635132\n",
      "Batch: 764, Loss : 0.7496408820152283\n",
      "Batch: 765, Loss : 0.6368177533149719\n",
      "Batch: 766, Loss : 0.6189326643943787\n",
      "Batch: 767, Loss : 0.5008600950241089\n",
      "Batch: 768, Loss : 0.6405323147773743\n",
      "Batch: 769, Loss : 0.724159300327301\n",
      "Batch: 770, Loss : 0.7822675704956055\n",
      "Batch: 771, Loss : 0.777959406375885\n",
      "Batch: 772, Loss : 0.5578423738479614\n",
      "Batch: 773, Loss : 0.7197760343551636\n",
      "Batch: 774, Loss : 0.6349110007286072\n",
      "Batch: 775, Loss : 0.5560476183891296\n",
      "Batch: 776, Loss : 0.6276631355285645\n",
      "Batch: 777, Loss : 0.4807799756526947\n",
      "Batch: 778, Loss : 0.6103381514549255\n",
      "Batch: 779, Loss : 0.5909134745597839\n",
      "Batch: 780, Loss : 0.5853034257888794\n",
      "Batch: 781, Loss : 0.5864917039871216\n",
      "Batch: 782, Loss : 0.6440345048904419\n",
      "Batch: 783, Loss : 0.6111915111541748\n",
      "Batch: 784, Loss : 0.6381796002388\n",
      "Batch: 785, Loss : 0.6048533916473389\n",
      "Batch: 786, Loss : 0.7948800325393677\n",
      "Batch: 787, Loss : 0.7359010577201843\n",
      "Batch: 788, Loss : 0.7818341851234436\n",
      "Batch: 789, Loss : 0.7830139994621277\n",
      "Batch: 790, Loss : 0.4901583194732666\n",
      "Batch: 791, Loss : 0.7393117547035217\n",
      "Batch: 792, Loss : 0.6206240057945251\n",
      "Batch: 793, Loss : 0.5093493461608887\n",
      "Batch: 794, Loss : 0.6644992232322693\n",
      "Batch: 795, Loss : 0.6052891612052917\n",
      "Batch: 796, Loss : 0.4864855408668518\n",
      "Batch: 797, Loss : 0.6263741254806519\n",
      "Batch: 798, Loss : 0.7828539609909058\n",
      "Batch: 799, Loss : 0.609865665435791\n",
      "Batch: 800, Loss : 0.697594940662384\n",
      "Batch: 801, Loss : 0.5230947136878967\n",
      "Batch: 802, Loss : 0.748450517654419\n",
      "Batch: 803, Loss : 0.5206251740455627\n",
      "Batch: 804, Loss : 0.7293807864189148\n",
      "Batch: 805, Loss : 0.7785860300064087\n",
      "Batch: 806, Loss : 0.6438400149345398\n",
      "Batch: 807, Loss : 0.8000273704528809\n",
      "Batch: 808, Loss : 0.8091323971748352\n",
      "Batch: 809, Loss : 0.6372702717781067\n",
      "Batch: 810, Loss : 0.6443788409233093\n",
      "Batch: 811, Loss : 0.5922212600708008\n",
      "Batch: 812, Loss : 0.6256951093673706\n",
      "Batch: 813, Loss : 0.5687094330787659\n",
      "Batch: 814, Loss : 0.7052819132804871\n",
      "Batch: 815, Loss : 0.6688369512557983\n",
      "Batch: 816, Loss : 0.5904251337051392\n",
      "Batch: 817, Loss : 0.6995391249656677\n",
      "Batch: 818, Loss : 0.6213527321815491\n",
      "Batch: 819, Loss : 0.742580235004425\n",
      "Batch: 820, Loss : 0.48781344294548035\n",
      "Batch: 821, Loss : 0.5921104550361633\n",
      "Batch: 822, Loss : 0.7097777724266052\n",
      "Batch: 823, Loss : 0.5856881737709045\n",
      "Batch: 824, Loss : 0.9057977795600891\n",
      "Batch: 825, Loss : 0.49263259768486023\n",
      "Batch: 826, Loss : 0.640594482421875\n",
      "Batch: 827, Loss : 0.4563542306423187\n",
      "Batch: 828, Loss : 0.6861488819122314\n",
      "Batch: 829, Loss : 0.8616870641708374\n",
      "Batch: 830, Loss : 0.6545523405075073\n",
      "Batch: 831, Loss : 0.4490295648574829\n",
      "Batch: 832, Loss : 0.6908265948295593\n",
      "Batch: 833, Loss : 0.5687651038169861\n",
      "Batch: 834, Loss : 0.7361218929290771\n",
      "Batch: 835, Loss : 0.6638309955596924\n",
      "Batch: 836, Loss : 0.6115628480911255\n",
      "Batch: 837, Loss : 0.7165715098381042\n",
      "Batch: 838, Loss : 0.8055444955825806\n",
      "Batch: 839, Loss : 0.6908993721008301\n",
      "Batch: 840, Loss : 0.7548239827156067\n",
      "Batch: 841, Loss : 0.6105538010597229\n",
      "Batch: 842, Loss : 0.48838192224502563\n",
      "Batch: 843, Loss : 0.6444940567016602\n",
      "Batch: 844, Loss : 0.7483074069023132\n",
      "Batch: 845, Loss : 0.6138123273849487\n",
      "Batch: 846, Loss : 0.6286162734031677\n",
      "Batch: 847, Loss : 0.6471248269081116\n",
      "Batch: 848, Loss : 0.7094143033027649\n",
      "Batch: 849, Loss : 0.6184859871864319\n",
      "Batch: 850, Loss : 0.8234073519706726\n",
      "Batch: 851, Loss : 0.596785843372345\n",
      "Batch: 852, Loss : 0.7047364115715027\n",
      "Batch: 853, Loss : 0.717204749584198\n",
      "Batch: 854, Loss : 0.5798110961914062\n",
      "Batch: 855, Loss : 0.7943759560585022\n",
      "Batch: 856, Loss : 0.7393946051597595\n",
      "Batch: 857, Loss : 0.6136762499809265\n",
      "Batch: 858, Loss : 0.6061061024665833\n",
      "Batch: 859, Loss : 0.56565260887146\n",
      "Batch: 860, Loss : 0.60664302110672\n",
      "Batch: 861, Loss : 0.4662274420261383\n",
      "Batch: 862, Loss : 0.6749910116195679\n",
      "Batch: 863, Loss : 0.7102918028831482\n",
      "Batch: 864, Loss : 0.5478458404541016\n",
      "Batch: 865, Loss : 0.6534010767936707\n",
      "Batch: 866, Loss : 0.6479744911193848\n",
      "Batch: 867, Loss : 0.7012450098991394\n",
      "Batch: 868, Loss : 0.7903860807418823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 869, Loss : 0.527176558971405\n",
      "Batch: 870, Loss : 0.49753671884536743\n",
      "Batch: 871, Loss : 0.5303326845169067\n",
      "Batch: 872, Loss : 0.702815592288971\n",
      "Batch: 873, Loss : 0.550304651260376\n",
      "Batch: 874, Loss : 0.5853625535964966\n",
      "Batch: 875, Loss : 0.8411471247673035\n",
      "Batch: 876, Loss : 0.5865780115127563\n",
      "Batch: 877, Loss : 0.6469947099685669\n",
      "Batch: 878, Loss : 0.5588973164558411\n",
      "Batch: 879, Loss : 0.49517878890037537\n",
      "Batch: 880, Loss : 0.5952535271644592\n",
      "Batch: 881, Loss : 0.6198689341545105\n",
      "Batch: 882, Loss : 0.6277535557746887\n",
      "Batch: 883, Loss : 0.9258030652999878\n",
      "Batch: 884, Loss : 0.646431028842926\n",
      "Batch: 885, Loss : 0.6362239122390747\n",
      "Batch: 886, Loss : 0.8353440761566162\n",
      "Batch: 887, Loss : 0.7380415797233582\n",
      "Batch: 888, Loss : 0.8363584280014038\n",
      "Batch: 889, Loss : 0.5739362835884094\n",
      "Batch: 890, Loss : 0.6972647905349731\n",
      "Batch: 891, Loss : 0.6996551752090454\n",
      "Batch: 892, Loss : 0.7093980312347412\n",
      "Batch: 893, Loss : 0.7814003229141235\n",
      "Batch: 894, Loss : 0.6296308636665344\n",
      "Batch: 895, Loss : 0.5360817313194275\n",
      "Batch: 896, Loss : 0.5018362998962402\n",
      "Batch: 897, Loss : 0.6824226379394531\n",
      "Batch: 898, Loss : 0.6909298896789551\n",
      "Batch: 899, Loss : 0.6936240792274475\n",
      "Batch: 900, Loss : 0.6948217153549194\n",
      "Batch: 901, Loss : 0.4846312701702118\n",
      "Batch: 902, Loss : 0.6193417310714722\n",
      "Batch: 903, Loss : 0.5785412192344666\n",
      "Batch: 904, Loss : 0.55265873670578\n",
      "Batch: 905, Loss : 0.5188391208648682\n",
      "Batch: 906, Loss : 0.7890339493751526\n",
      "Batch: 907, Loss : 0.6565383672714233\n",
      "Batch: 908, Loss : 0.7198598384857178\n",
      "Batch: 909, Loss : 0.5977882146835327\n",
      "Batch: 910, Loss : 0.7209411263465881\n",
      "Batch: 911, Loss : 0.6872411966323853\n",
      "Batch: 912, Loss : 0.6171932220458984\n",
      "Batch: 913, Loss : 0.5216043591499329\n",
      "Batch: 914, Loss : 0.6802865266799927\n",
      "Batch: 915, Loss : 0.5904698967933655\n",
      "Batch: 916, Loss : 0.48624494671821594\n",
      "Batch: 917, Loss : 0.5816711187362671\n",
      "Batch: 918, Loss : 0.6011973023414612\n",
      "Batch: 919, Loss : 0.7041201591491699\n",
      "Batch: 920, Loss : 0.7075082063674927\n",
      "Batch: 921, Loss : 0.5541337132453918\n",
      "Batch: 922, Loss : 0.6058877110481262\n",
      "Batch: 923, Loss : 0.5957155227661133\n",
      "Batch: 924, Loss : 0.8011897206306458\n",
      "Batch: 925, Loss : 0.6127726435661316\n",
      "Batch: 926, Loss : 0.6980392932891846\n",
      "Batch: 927, Loss : 0.4888012409210205\n",
      "Batch: 928, Loss : 0.5607506632804871\n",
      "Batch: 929, Loss : 0.46086227893829346\n",
      "Batch: 930, Loss : 0.7303630113601685\n",
      "Batch: 931, Loss : 0.6144898533821106\n",
      "Batch: 932, Loss : 0.5438396334648132\n",
      "Batch: 933, Loss : 0.5324358940124512\n",
      "Batch: 934, Loss : 0.7097559571266174\n",
      "Batch: 935, Loss : 0.6100807189941406\n",
      "Batch: 936, Loss : 0.4964296519756317\n",
      "Batch: 937, Loss : 0.6757065057754517\n",
      "Batch: 938, Loss : 0.7354550957679749\n",
      "Training loss: 1.0541102124620347\n"
     ]
    }
   ],
   "source": [
    "model = FMNIST()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    cum_loss = 0\n",
    "    bs_num = 0\n",
    "\n",
    "    for bs_num, (images, labels) in enumerate(trainloader, 1):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        cum_loss += loss.item()\n",
    "        print(f'Batch: {bs_num}, Loss : {loss.item()}')\n",
    "     \n",
    "    print(f\"Training loss: {cum_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-OVDFUnzFGpr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hWf1SWuiFGmn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OQ_QUMXLFGjr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "03_04_Using_Optimizers.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
